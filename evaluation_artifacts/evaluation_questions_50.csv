id|QUESTION|CONTEXT|Expected Answer
1|Does SageMaker provide any free ML environments?|SageMaker includes the following machine learning environments: SageMaker Canvas: An auto ML service that gives people with no coding experience the ability to build models and make predictions with them. SageMaker Studio:   An integrated machine learning environment where you can build, train, deploy, and analyze your models all in the same application. SageMaker Studio Lab: A free service that gives customers access to AWS compute resources in an environment based on open-source JupyterLab. RStudio on Amazon SageMaker: An integrated development environment for R, with a console, syntax-highlighting editor that supports direct code execution, and tools for plotting, history, debugging and workspace management.|Yes, SageMaker Studio Lab is a free service that gives customers access to AWS compute resources for machine learning.
2|What are some of the ways to access JumpStart?|SageMaker JumpStart provides pretrained, open-source models for a wide range of problem types to help you get started with machine learning. You can incrementally train and tune these models before deployment. JumpStart also provides solution templates that set up infrastructure for common use cases, and executable example notebooks for machine learning with SageMaker. You can access the pretrained models, solution templates, and examples through the JumpStart landing page in Amazon SageMaker Studio. You can also access JumpStart models using the SageMaker Python SDK.|You can access JumpStart through the JumpStart landing page in Amazon SageMaker Studio. You can also access JumpStart models using the SageMaker Python SDK.
3|Can Ground Truth help me save my labelling costs? If so, how?|To train a machine learning model, you need a large, high-quality, labeled dataset. You can label your data using Amazon SageMaker Ground Truth. Choose from one of the Ground Truth built-in task types or create your own custom labeling workflow. To improve the accuracy of your data labels and reduce the total cost of labeling your data, use Ground Truth enhanced data labeling features like automated data labeling and annotation consolidation. |Yes. Ground Truth can help reduce labeling costs through automated data labeling and annotation consolidation.
4|Which SageMaker feature can help me prepare my datasets for model training?|To analyze data and evaluate machine learning models on Amazon SageMaker, use Amazon SageMaker Processing. With Processing, you can use a simplified, managed experience on SageMaker to run your data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation. Your input data must be stored in an Amazon S3 bucket. Alternatively, you can use Amazon Athena or Amazon Redshift as input sources.|SageMaker Processing
5|Can SageMaker alert me if a model's accuracy goes down? if so, how?|Amazon SageMaker Model Monitor monitors the quality of Amazon SageMaker machine learning models in production. You can set up continuous monitoring with a real-time endpoint (or a batch transform job that runs regularly), or on-schedule monitoring for asynchronous batch transform jobs. With Model Monitor, you can set alerts that notify you when there are deviations in the model quality. Model Monitor provides the following types of monitoring: Monitor drift in data quality, Monitor drift in model quality metrics, such as accuracy. Monitor bias in your model's predictions, Monitor drift in feature attribution.|Yes. SageMaker Model Monitor can monitor drift in model quality metrics, such as accuracy. You can set alerts that notify you when there are deviations in the model quality.
6|Who is responsible for security of your data on AWS?|Security is a shared responsibility between AWS and you. The shared responsibility model describes this as security of the cloud and security in the cloud: Security of the cloud – AWS is responsible for protecting the infrastructure that runs AWS services in the AWS Cloud. AWS also provides you with services that you can use securely. Security in the cloud – Your responsibility is determined by the AWS service that you use. You are also responsible for other factors including the sensitivity of your data, your company’s requirements, and applicable laws and regulations. |You are responsible for security of your data on AWS.
7|How can I track who made a SageMaker API call?|Monitoring is an important part of maintaining the reliability, availability, and performance of SageMaker and your other AWS solutions. AWS provides the following monitoring tools: Amazon CloudWatch monitors your AWS resources and the applications that you run on AWS in real time. Amazon CloudWatch Logs enables you to monitor, store, and access your log files. AWS CloudTrail captures API calls and related events made by or on behalf of your AWS account. CloudWatch Events delivers a near real-time stream of system events that describe changes in AWS resources. |AWS CloudTrail captures API calls and related events made by or on behalf of your AWS account.
8|What type of model monitoring is available through SageMaker?|Amazon SageMaker Model Monitor monitors the quality of Amazon SageMaker machine learning models in production. You can set up continuous monitoring with a real-time endpoint (or a batch transform job that runs regularly), or on-schedule monitoring for asynchronous batch transform jobs. With Model Monitor, you can set alerts that notify you when there are deviations in the model quality. Early and proactive detection of these deviations enables you to take corrective actions, such as retraining models, auditing upstream systems, or fixing quality issues without having to monitor models manually or build additional tooling. You can use Model Monitor prebuilt monitoring capabilities that do not require coding. You also have the flexibility to monitor models by coding to provide custom analysis.|SageMaker Model Monitor provides continuous monitoring with real-time endpoints or regular batch transform jobs, as well as on-schedule monitoring for asynchronous batch transform jobs.
9|What SageMaker service can be used for feature engineering?|Amazon SageMaker Data Wrangler (Data Wrangler) is a feature of Amazon SageMaker Studio that provides an end-to-end solution to import, prepare, transform, featurize, and analyze data. You can integrate a Data Wrangler data preparation flow into your machine learning (ML) workflows to simplify and streamline data pre-processing and feature engineering using little to no coding. You can also add your own Python scripts and transformations to customize workflows.|Amazon SageMaker Data Wrangler
10|What is the default instance type used by Data Wrangler?|When you create a Data Wrangler flow in Amazon SageMaker Studio, Data Wrangler uses an Amazon EC2 instance to run the analyses and transformations in your flow. By default, Data Wrangler uses the m5.4xlarge instance. m5 instances are general purpose instances that provide a balance between compute and memory. You can use m5 instances for a variety of compute workloads. Data Wrangler also gives you the option of using r5 instances. r5 instances are designed to deliver fast performance that processes large datasets in memory.|The default instance type used by Data Wrangler is m5.4xlarge.
11|How do I get the output from Data Wrangler?|A Data Wrangler flow is the series of data preparation steps that you've performed on your data. In your data preparation, you perform one or more transformations to your data. Each transformation is done using a transform step. The flow has a series of nodes that represent the import of your data and the transformations that you've performed. You can export your data transformations to the following: Amazon S3, SageMaker Pipelines, Amazon SageMaker Feature Store, Python Code|You can export your data transformations from Data Wrangler to Amazon S3, SageMaker Pipelines, Amazon SageMaker Feature Store, or Python Code.
12|Where is a Data Wrangler Flow stored?|When you query data from Athena or Amazon Redshift, the queried dataset is automatically stored in the default SageMaker S3 bucket for the AWS Region in which you are using Studio. Additionally, when you export a Jupyter Notebook from Amazon SageMaker Data Wrangler and run it, your data flows, or .flow files, are saved to the same default bucket, under the prefix data_wrangler_flows.|The Data Wrangler Flow is stored in the default SageMaker S3 bucket for the AWS Region, under the prefix data_wrangler_flows.
13|Which bucket does Data Wrangler store resources?|Resources that Data Wrangler uploads to your default SageMaker S3 bucket in the AWS Region you are using Studio in include the following:Queried Amazon Redshift results. These are stored under the redshift/ prefix.Queried Athena results. These are stored under the athena/ prefix. The .flow files uploaded to Amazon S3 when you run an exported Jupyter Notebook Data Wrangler produces. These are stored under the data_wrangler_flows/ prefix.|The default SageMaker S3 bucket in the AWS Region you are using Studio in.
14|How can I use a Data Wrangler flow on different data sets?|For Amazon Simple Storage Service (Amazon S3) data sources, you can create and use parameters. A parameter is a variable that you've saved in your Data Wrangler flow. Its value can be any portion of the data source's Amazon S3 path. Use parameters to quickly change the data that you're importing into a Data Wrangler flow or exporting to a processing job. You can also use parameters to select and import a specific subset of your data.|You can create and use parameters to quickly change the data that you're importing into a Data Wrangler flow.
15|Can I use my own Docker image with SageMaker Processing?|Amazon SageMaker spins up a Processing job. Amazon SageMaker takes your script, copies your data from Amazon Simple Storage Service (Amazon S3), and then pulls a processing container. The processing container image can either be an Amazon SageMaker built-in image or a custom image that you provide. The underlying infrastructure for a Processing job is fully managed by Amazon SageMaker. Cluster resources are provisioned for the duration of your job, and cleaned up when a job completes. The output of the Processing job is stored in the Amazon S3 bucket you specified. |Yes, you can use your own Docker image with SageMaker Processing. The processing container image can either be an Amazon SageMaker built-in image or a custom image that you provide.
16|Why do I need a Feature Store?|Feature Store: Storage and data management layer for machine learning (ML) features. Serves as the single source of truth to store, retrieve, remove, track, share, discover, and control access to features. In the following example diagram, the Feature Store is a store for your feature groups, which contains your ML data, and provides additional services. |You need a Feature Store as the single source of truth to store, retrieve, remove, track, share, discover, and control access to features for machine learning.
17|How can I save data in a Feature Store?|You can ingest data into feature groups in Feature Store in two ways: streaming or in batches. When you ingest data through streaming, a collection of records are pushed to Feature Store by calling a synchronous PutRecord API call. Feature Store can process and ingest data in batches. You can author features using Amazon SageMaker Data Wrangler, create feature groups in Feature Store and ingest features in batches using a SageMaker Processing job with a notebook exported from Data Wrangler. This mode allows for batch ingestion into the offline store. It also supports ingestion into the online store if the feature group is configured for both online and offline use.|You can save data in a Feature Store in AWS SageMaker by ingesting data in streaming mode using the PutRecord API or in batches using a SageMaker Processing job with a notebook exported from SageMaker Data Wrangler.
18|What can SageMaker Data Wrangler do without writing any code?|SageMaker Data Wrangler, without writing a single line of code, can automatically transform your data into new features. SageMaker Data Wrangler offers a selection of preconfigured data transforms, impute missing data, one-hot encoding, dimensionality reduction using principal components analysis (PCA), as well as time-series specific transformers. For example, you can convert a text field column into a numerical column in a single step. You can also author a code snippet from the SageMaker Data Wrangler library of snippets.|SageMaker Data Wrangler can automatically transform data into new features, impute missing data, one-hot encode, perform dimensionality reduction using PCA, and apply time-series specific transformers without writing any code.
19|What are my options for running a distributed training job?|With only a few lines of additional code in your PyTorch and TensorFlow training scripts, SageMaker will automatically apply data parallelism or model parallelism for you, allowing you to develop and deploy your models faster. SageMaker will determine the best approach to split your model by using graph partitioning algorithms to balance the computation of each GPU while minimizing the communication between GPU instances. SageMaker also optimizes your distributed training jobs through algorithms that fully utilize the AWS compute and network in order to achieve near-linear scaling efficiency, which allows you to complete training faster than manual open-source implementations.|With SageMaker, you have the option to automatically apply data parallelism or model parallelism to distributed training jobs. SageMaker determines the best approach and handles the underlying implementation.
20|Can I use spot instances for my training jobs?|You enable the Managed Spot Training option when submitting your training jobs and you also specify how long you want to wait for Spot capacity. SageMaker will then use Amazon EC2 Spot Instances to run your job and manages the Spot capacity. You have full visibility into the status of your training jobs, both while they are running and while they are waiting for capacity.|Yes
21|What model deployment options does SageMaker provide?|After you build and train models, SageMaker provides three options to deploy them so you can start making predictions. Real-time inference is suitable for workloads with millisecond latency requirements, payload sizes up to 6 MB, and processing times of up to 60 seconds. Batch transform is ideal for offline predictions on large batches of data that are available up front. Asynchronous inference is designed for workloads that do not have sub-second latency requirements, payload sizes up to 1 GB, and processing times of up to 15 minutes. |SageMaker provides three model deployment options: real-time inference, batch transform, and asynchronous inference.
22|What is the key benefit of using SageMaker Serverless Inference for deploying ML models?|SageMaker Serverless Inference is a purpose-built serverless model serving option that makes it easy to deploy and scale ML models. SageMaker Serverless Inference endpoints automatically start the compute resources and scale them in and out depending on traffic, eliminating the need for you to choose instance type, run provisioned capacity, or manage scaling. You can optionally specify the memory requirements for your serverless inference endpoint. You pay only for the duration of running the inference code and the amount of data processed, not for idle periods.|SageMaker Serverless Inference automatically scales compute resources up and down based on traffic, eliminating the need to manually manage capacity and scaling.
23|What is SageMaker shadow testing?|SageMaker helps you run shadow tests to evaluate a new ML model before production release by testing its performance against the currently deployed model. SageMaker deploys the new model in shadow mode alongside the current production model and mirrors a user-specified portion of the production traffic to the new model. It optionally logs the model inferences for offline comparison. It also provides a live dashboard with a comparison of key performance metrics, such as latency and error rate, between the production and shadow models to help you decide whether to promote the new model to production.|SageMaker shadow testing evaluates a new ML model before production release by testing its performance against the currently deployed model.
24|What is the key benefit of using SageMaker Inference Recommender?|SageMaker Inference Recommender reduces the time required to get ML models in production by automating performance benchmarking and tuning model performance across SageMaker ML instances. You can now use SageMaker Inference Recommender to deploy your model to an endpoint that delivers the best performance and minimizes cost. You can get started with SageMaker Inference Recommender in minutes while selecting an instance type and get recommendations for optimal endpoint configurations within hours, eliminating weeks of manual testing and tuning time. With SageMaker Inference Recommender, you pay only for the SageMaker ML instances used during load testing, and there are no additional charges.|SageMaker Inference Recommender automates performance benchmarking and tuning to reduce the time required to get ML models into production.
25|What steps do I need to take on the edge devices before deploying a model with SageMaker Edge Manager?|To get started with SageMaker Edge Manager, you need to compile and package your trained ML models in the cloud, register your devices, and prepare your devices with the SageMaker Edge Manager SDK. To prepare your model for deployment, SageMaker Edge Manager uses SageMaker Neo to compile your model for your target edge hardware. Once a model is compiled, SageMaker Edge Manager signs the model with an AWS generated key, then packages the model with its runtime and your necessary credentials to get it ready for deployment. On the device side, you register your device with SageMaker Edge Manager, download the SageMaker Edge Manager SDK, and then follow the instructions to install the SageMaker Edge Manager agent on your devices. The tutorial notebook provides a step-by-step example of how you can prepare the models and connect your models on edge devices with SageMaker Edge Manager.|Register your device with SageMaker Edge Manager, download the SageMaker Edge Manager SDK, and then follow the instructions to install the SageMaker Edge Manager agent on your devices.
26|How am I charged for SageMaker?|You pay for ML compute, storage, and data processing resources that you use for hosting the notebook, training the model, performing predictions, and logging the outputs. With SageMaker, you can select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments.|You pay only for the ML compute, storage, and data processing resources that you use in SageMaker, as you use them. There are no minimum fees or upfront commitments.
27|What is the main benefit of using SageMaker Studio?|SageMaker Studio provides a single, web-based visual interface where you can perform all ML development steps. SageMaker Studio gives you complete access, control, and visibility into each step required to prepare data and build, train, and deploy models. You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive. All ML development activities including notebooks, experiment management, automatic model creation, debugging and profiling, and model drift detection can be performed within the unified SageMaker Studio visual interface.|SageMaker Studio provides a single, web-based visual interface that gives you complete access, control, and visibility into each step required to prepare data and build, train, and deploy models.
28|Which SageMaker feature can help with documenting models and how?|SageMaker Model Cards helps you centralize and standardize model documentation throughout the ML lifecycle by creating a single source of truth for model information. SageMaker Model Cards auto-populates training details to accelerate the documentation process. You can also add details such as the purpose of the model and the performance goals. You can attach model evaluation results to your model card and provide visualizations to gain key insights into model performance. SageMaker Model Cards can easily be shared with others by exporting to a PDF format.|SageMaker Model Cards helps centralize and standardize model documentation by creating a single source of truth for model information.
29|How is the usage of SageMaker Canvas charged?|With SageMaker Canvas, you pay based on usage. SageMaker Canvas lets you interactively ingest, explore, and prepare your data from multiple sources, train highly accurate ML models with your data, and generate predictions. There are two components that determine your bill: session charges based on the number of hours for which SageMaker Canvas is used or logged into, and charges for training the model based on the size of the dataset used to build the model.|With SageMaker Canvas, you pay based on usage. There are charges for the number of hours SageMaker Canvas is used or logged into, and charges for training the model based on the size of the dataset used.
30|How to start a terminal session on a running endpoint?|Amazon SageMaker allows you to securely connect to the Docker containers on which your models are deployed on for Inference using AWS Systems Manager (SSM). This gives you shell level access to the container so that you can debug the processes running within the container and log commands and responses with Amazon CloudWatch. |You can securely connect to the Docker containers on which your models are deployed for Inference using AWS Systems Manager (SSM). This gives you shell level access to the container.
31|Which model servers are supported by SageMaker?|You can deploy your models on SageMaker using popular model servers. TorchServe is the recommended model server for PyTorch, preinstalled in the AWS PyTorch Deep Learning Container (DLC). DJL Serving is a high performance universal stand-alone model serving solution. It takes a deep learning model, several models, or workflows and makes them available through an HTTP endpoint. Triton Inference Server is an open source inference serving software that streamlines AI inference. With Triton, you can deploy any model built with multiple deep learning and machine learning frameworks, including TensorRT, TensorFlow, PyTorch, ONNX, OpenVINO, Python, RAPIDS FIL, and more.|TorchServe, DJL Serving, and Triton Inference Server are supported model servers in SageMaker.
32|How many models can I load on an edge device?|SageMaker Edge Manager agent can load multiple models at a time and make inference with loaded models on edge devices. The number of models the agent can load is determined by the available memory on the device. The agent validates the model signature and loads into memory all the artifacts produced by the edge packaging job. |I don't know. The context does not specify how many models can be loaded on an edge device. It only says the number is determined by the available memory on the device.
33|How to safely update my running model?|Changing or deleting model artifacts or changing inference code after deploying a model produces unpredictable results. If you need to change or delete model artifacts or change inference code, modify the endpoint by providing a new endpoint configuration. Once you provide the new endpoint configuration, you can change or delete the model artifacts corresponding to the old endpoint configuration.|Provide a new endpoint configuration.
34|How can I reduce invocation latency?|The application latency is made up of two primary components: infrastructure or overhead latency and model inference latency. You can reduce the latency for real-time inferences with SageMaker using an AWS PrivateLink deployment. With AWS PrivateLink, you can privately access all SageMaker API operations from your Virtual Private Cloud (VPC) in a scalable manner by using interface VPC endpoints. An interface VPC endpoint is an elastic network interface in your subnet with private IP addresses that serves as an entry point for all SageMaker API calls.|By using an AWS PrivateLink deployment to privately access SageMaker API operations from your VPC with interface VPC endpoints.
35|What is the best way to reduce my inference costs?|SageMaker Inference has over 70 instance types and sizes that can be used to deploy ML models including AWS Inferentia and Graviton chipsets that are optimized for ML. Choosing the right instance for your model helps ensure you have the most performant instance at the lowest cost for your models. By using Inference Recommender, you can quickly compare different instances to understand the performance of the model and the costs. With these results, you can choose the instance to deploy with the best return on investment.|Use SageMaker Inference Recommender to choose the most performant instance at the lowest cost for your models.
36|How can I ensure I have enough capacity for my models?|Autoscaling is an out-of-the-box feature that monitors your workloads and dynamically adjusts the capacity to maintain steady and predictable performance at the possible lowest cost. When the workload increases, autoscaling brings more instances online. When the workload decreases, autoscaling removes unnecessary instances, helping you reduce your compute cost.|By enabling autoscaling, which monitors workloads and adjusts capacity to maintain performance at the lowest cost.
37|Does SageMaker support any model parallelism and inference optimization libraries?|SageMaker maintains deep learning containers (DLCs) with popular open source libraries for hosting large models such as GPT, T5, OPT, BLOOM, and Stable Diffusion on AWS infrastructure. With these DLCs you can use third party libraries such as DeepSpeed, Accelerate, and FasterTransformer to partition model parameters using model parallelism techniques to leverage the memory of multiple GPUs for inference.|Yes, SageMaker supports model parallelism and inference optimization libraries like DeepSpeed, Accelerate, and FasterTransformer.
38|Are there any parameters to support large model inference on SageMaker?|You can customize the following parameters to facilitate low-latency large model inference (LMI) with SageMaker: Maximum Amazon EBS volume size on the instance (VolumeSizeInGB) – If the size of the model is larger than 30 GB and you are using an instance without a local disk, you should increase this parameter to be slightly larger than the size of your model. Health check timeout quota (ContainerStartupHealthCheckTimeoutInSeconds) – If your container is correctly set up and the CloudWatch logs indicate a health check timeout, you should increase this quota so the container has enough time to respond to health checks. Model download timeout quota (ModelDataDownloadTimeoutInSeconds) – If the size of your model is larger than 40 GB, then you should increase this quota to provide sufficient time to download the model from Amazon S3 to the instance.|Yes, there are parameters to support large model inference on SageMaker: - Maximum Amazon EBS volume size on the instance (VolumeSizeInGB) \n\n- Health check timeout quota (ContainerStartupHealthCheckTimeoutInSeconds)\n\n- Model download timeout quota (ModelDataDownloadTimeoutInSeconds)
39|How can I handle low throughput of large models?|Even with model parallelism, large models can still have poor performance such as high latency or low throughput. First, if your model is supported, consider using the DeepSpeed engine that includes optimized kernels for inference. This can improve performance by up to 3 times. Next, consider using a more powerful instance type, such as a p4d.24xlarge instance. Finally, consider using a smaller model. To achieve the best performance and lowest cost, try to find the smallest model that meets the accuracy bar for your use case, as more parameters can result in higher latency, lower throughput, or higher cost. Consider fine-tuning a smaller model to improve accuracy. |Use DeepSpeed engine if supported. Upgrade to more powerful instance type like p4d.24xlarge. Use a smaller model that meets accuracy needs. Fine-tune a smaller model.
40|What types of deployments are supported by SAgeMaker to update models in production?|Deployment guardrails are a set of model deployment options in Amazon SageMaker Inference to update your machine learning models in production. Using the fully managed deployment options, you can control the switch from the current model in production to a new one. Traffic shifting modes in blue/green deployments, such as canary and linear, give you granular control over the traffic shifting process from your current model to the new one during the course of the update. There are also built-in safeguards such as auto-rollbacks that help you catch issues early and automatically take corrective action before they significantly impact production.      With Rolling Deployments: You can update your endpoint as SageMaker incrementally provisions capacity and shifts traffic to a new fleet in steps of a batch size that you specify. Instances on the new fleet are updated with the new deployment configuration, and if no CloudWatch alarms trip during the baking period, then SageMaker cleans up instances on the old fleet. |SageMaker supports blue/green and rolling deployments to update models in production.
41|How can I monitor the performance of my running models?|After creating a SageMaker Hosting endpoint, you can monitor your endpoint using Amazon CloudWatch, which collects raw data and processes it into readable, near real-time metrics. Using these metrics, you can access historical information and gain a better perspective on how your endpoint is performing.|You can monitor the performance of your running models using Amazon CloudWatch metrics for your SageMaker endpoint.
42|What are the advantages of using SageMaker Python SDK for creating a pipeline?|The SageMaker Python SDK is not required to create a SageMaker pipeline. You can also use boto3 or AWS CloudFormation. Creating a pipeline requires a pipeline definition, which is a JSON object that defines each step of the pipeline. The SageMaker SDK offers a simple way to construct the pipeline definition, which you can use with any of the APIs previously mentioned to create the pipeline itself. Without using the SDK, users have to write the raw JSON definition to create the pipeline without any of the error checks provided by the SageMaker Python SDK|The SageMaker Python SDK offers a simple way to construct the pipeline definition and provides error checking, compared to writing raw JSON or using boto3/CloudFormation directly.
43|What’s the best way to reproduce my model in SageMaker?|SageMaker’s Lineage Tracking service works in the backend to track all the metadata associated with your model training and deployment workflows. This includes your training jobs, datasets used, pipelines, endpoints, and the actual models. You can query the lineage service at any point to find the exact artifacts used to train a model. Using those artifacts, you can recreate the same ML workflow to reproduce the model as long as you have access to the exact dataset that was used. A trial component tracks the training job. This trial component has all the parameters used as part of the training job. If you don’t need to rerun the entire workflow, you can reproduce the training job to derive the same model. |You can query the SageMaker Lineage Tracking service to find the exact artifacts used to train your model. Then you can recreate the same ML workflow with the same dataset to reproduce the model.
44|What does Amazon SageMaker Model Monitor allow customers to do?|Customers can monitor model behavior along four dimensions - Data quality, Model quality, Bias drift, and Feature Attribution drift through Amazon SageMaker Model Monitor and SageMaker Clarify. Model Monitor continuously monitors the quality of Amazon SageMaker machine learning models in production. This includes monitoring drift in data quality and model quality metrics such as accuracy and RMSE. SageMaker Clarify bias monitoring helps data scientists and ML engineers monitor bias in model’s prediction and feature attribution drift.|Monitor model behavior along four dimensions - Data quality, Model quality, Bias drift, and Feature Attribution drift.
45|Can I use SageMaker Model monitor for NLP models?|Amazon SageMaker's prebuilt container supports tabular datasets. If you want to monitor NLP and CV models, you can bring your own container by leveraging the extension points provided by Model Monitor.|Yes, you can use SageMaker Model Monitor for NLP models by bringing your own container.
46|Which governance tools are provided by SageMaker?|Model governance is a framework that gives systematic visibility into machine learning (ML) model development, validation, and usage. Amazon SageMaker provides purpose-built ML governance tools for managing control access, activity tracking, and reporting across the ML lifecycle. Manage least-privilege permissions for ML practitioners using Amazon SageMaker Role Manager, create detailed model documentation using Amazon SageMaker Model Cards, and gain visibility into your models with centralized dashboards using Amazon SageMaker Model Dashboard.|SageMaker provides Role Manager for managing permissions, Model Cards for model documentation, and Model Dashboard for visibility into models.
47|Which tools are provided by SageMaker for debugging?|Amazon SageMaker provides two debugging tools to help identify such convergence issues and gain visibility into your models. To offer a greater compatiblity with the open-source community tools within the SageMaker Training platform, SageMaker hosts TensorBoard as an application in SageMaker Domain. You can bring your training jobs to SageMaker and keep using the TensorBoard summary writer to collect the model output tensors. Amazon SageMaker Debugger is a capability of SageMaker that provides tools to register hooks to callbacks to extract model output tensors and save them in Amazon Simple Storage Service. It provides built-in rules for detecting model convergence issues, such as overfitting, saturated activation functions, vanishing gradients, and more.|Amazon SageMaker provides TensorBoard and SageMaker Debugger for debugging.
48|How can I create a profile of the compute resources used by a training job?|Amazon SageMaker Profiler is a capability of Amazon SageMaker that provides a detailed view into the AWS compute resources provisioned during training deep learning models on SageMaker. It focuses on profiling the CPU and GPU usage, kernel runs on GPUs, kernel launches on CPUs, sync operations, memory operations across CPUs and GPUs, latencies between kernel launches and corresponding runs, and data transfer between CPUs and GPUs.|You can use Amazon SageMaker Profiler to create a detailed profile of the CPU, GPU, memory, and data transfer usage during SageMaker training jobs.
49|How does SageMaker support checkpointing during model training?|The SageMaker training mechanism uses training containers on Amazon EC2 instances, and the checkpoint files are saved under a local directory of the containers (the default is /opt/ml/checkpoints). SageMaker provides the functionality to copy the checkpoints from the local path to Amazon S3 and automatically syncs the checkpoints in that directory with Amazon S3.|SageMaker saves checkpoints to a local directory on the training container during training. It provides functionality to copy these checkpoints to Amazon S3 to sync them.